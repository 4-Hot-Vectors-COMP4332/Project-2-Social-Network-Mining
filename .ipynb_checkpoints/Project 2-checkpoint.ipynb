{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting networkx\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/41/8f/dd6a8e85946def36e4f2c69c84219af0fa5e832b018c970e92f2ad337e45/networkx-2.4-py3-none-any.whl (1.6 MB)\n",
      "Collecting gensim\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f7/07/79584906b5a513842e5a7b9183ee5728c0bc0c3533e0686f7637f1ece6e8/gensim-3.8.2-cp36-cp36m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\yfana\\anaconda3\\envs\\comp4332\\lib\\site-packages (from networkx) (4.4.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\yfana\\anaconda3\\envs\\comp4332\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5d/13/a2db017db801d0157fdc41814658396e6ae398d06adf69d73df1c8175b5d/smart_open-1.11.1.tar.gz (105 kB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\yfana\\anaconda3\\envs\\comp4332\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\yfana\\anaconda3\\envs\\comp4332\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: requests in c:\\users\\yfana\\anaconda3\\envs\\comp4332\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Collecting boto\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting boto3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8c/b9/d9da1f57c0535afacbdf20f3f0ce0c6cdae1b8adab0fd48459c6d72222e5/boto3-1.12.46-py2.py3-none-any.whl (128 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\yfana\\anaconda3\\envs\\comp4332\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yfana\\anaconda3\\envs\\comp4332\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\yfana\\anaconda3\\envs\\comp4332\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\yfana\\anaconda3\\envs\\comp4332\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a3/43/1e939e1fcd87b827fe192d0c9fc25b48c5b3368902bfb913de7754b0dc03/jmespath-0.9.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.16.0,>=1.15.46\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6c/21/d015b86c8fa291f78e8fe7d7cf0a60645989efa5db5e8302e2b7ccf9539a/botocore-1.15.46-py2.py3-none-any.whl (6.1 MB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\yfana\\anaconda3\\envs\\comp4332\\lib\\site-packages (from botocore<1.16.0,>=1.15.46->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-1.11.1-py3-none-any.whl size=95261 sha256=e74fc6649b5efe41dbe5a996f68262cd77428ba55543a547ab7fedcfc8cf1a23\n",
      "  Stored in directory: c:\\users\\yfana\\appdata\\local\\pip\\cache\\wheels\\8a\\c2\\d3\\16192a758e3f7d7661e23d86dc92a65f2e0a8f98600140f24d\n",
      "Successfully built smart-open\n",
      "Installing collected packages: networkx, boto, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto-2.49.0 boto3-1.12.46 botocore-1.15.46 docutils-0.15.2 gensim-3.8.2 jmespath-0.9.5 networkx-2.4 s3transfer-0.3.3 smart-open-1.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load networks into memory. Usually networks are organized as pairs of nodes. And sometimes different edges have different weights. Hence, we use networkx.DiGraph to store such structure information and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    read edges from an edge file\n",
    "    \"\"\"\n",
    "    edges = list()\n",
    "    df = pd.read_csv(file_name)\n",
    "    for idx, row in df.iterrows():\n",
    "        user_id, friends = row[\"user_id\"], eval(row[\"friends\"])\n",
    "        for friend in friends:\n",
    "            # add each friend relation as an edge\n",
    "            edges.append((user_id, friend))\n",
    "    edges = sorted(edges)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def generate_false_edges(true_edges, num_false_edges=20732):\n",
    "    \"\"\"\n",
    "    generate false edges given true edges\n",
    "    \"\"\"\n",
    "    nodes = list(set(chain.from_iterable(true_edges)))\n",
    "    true_edges = set(true_edges)\n",
    "    false_edges = set()\n",
    "    \n",
    "    while len(false_edges) < num_false_edges:\n",
    "        # randomly sample two different nodes and check whether the pair exisit or not\n",
    "        head, tail = np.random.choice(nodes, 2)\n",
    "        if head != tail and (head, tail) not in true_edges and (head, tail) not in false_edges:\n",
    "            false_edges.add((head, tail))    \n",
    "    false_edges = sorted(false_edges)\n",
    "    \n",
    "    return false_edges\n",
    "\n",
    "def construct_graph_from_edges(edges):\n",
    "    \"\"\"\n",
    "    generate a directed graph object given true edges\n",
    "    DiGraph documentation: https://networkx.github.io/documentation/stable/reference/classes/digraph.html\n",
    "    \"\"\"\n",
    "    # convert a list of edges {(u, v)} to a list of edges with weights {(u, v, w)}\n",
    "    edge_weight = defaultdict(float)\n",
    "    for e in edges:\n",
    "        edge_weight[e] += 1.0\n",
    "    weighed_edge_list = list()\n",
    "    for e in sorted(edge_weight.keys()):\n",
    "        weighed_edge_list.append((e[0], e[1], edge_weight[e]))\n",
    "        \n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_weighted_edges_from(weighed_edge_list)\n",
    "    \n",
    "    print(\"number of nodes:\", graph.number_of_nodes())\n",
    "    print(\"number of edges:\", graph.number_of_edges())\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random walk generators or random walkers yield random walks that contain both local and higher-order neighborhood information. However, naive non-uniform sampling is very slow, which requires O(n) time complexity. Here alias sampling can reduce the time complexity to O(1) with O(n) space. If you are interested, please see the following blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_setup(probs):\n",
    "    \"\"\"\n",
    "    compute utility lists for non-uniform sampling from discrete distributions.\n",
    "    details: https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "    \"\"\"\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "\n",
    "    smaller = list()\n",
    "    larger = list()\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K * prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q\n",
    "\n",
    "def get_alias_node(graph, node):\n",
    "    \"\"\"\n",
    "    get the alias node setup lists for a given node.\n",
    "    \"\"\"\n",
    "    # get the unnormalized probabilities with the first-order information\n",
    "    unnormalized_probs = list()\n",
    "    for nbr in graph.neighbors(node):\n",
    "        unnormalized_probs.append(graph[node][nbr]['weight'])\n",
    "    unnormalized_probs = np.array(unnormalized_probs)\n",
    "    if len(unnormalized_probs) > 0:\n",
    "        normalized_probs = unnormalized_probs / unnormalized_probs.sum()\n",
    "    else:\n",
    "        normalized_probs = unnormalized_probs\n",
    "        \n",
    "    return alias_setup(normalized_probs)\n",
    "    \n",
    "def get_alias_edge(graph, src, dst, p=1, q=1):\n",
    "    \"\"\"\n",
    "    get the alias edge setup lists for a given edge.\n",
    "    \"\"\"\n",
    "    # get the unnormalized probabilities with the second-order information\n",
    "    unnormalized_probs = list()\n",
    "    for dst_nbr in graph.neighbors(dst):\n",
    "        if dst_nbr == src: # distance is 0\n",
    "            unnormalized_probs.append(graph[dst][dst_nbr]['weight'] / p)\n",
    "        elif graph.has_edge(dst_nbr, src): # distance is 1\n",
    "            unnormalized_probs.append(graph[dst][dst_nbr]['weight'])\n",
    "        else: # distance is 2\n",
    "            unnormalized_probs.append(graph[dst][dst_nbr]['weight'] / q)\n",
    "    unnormalized_probs = np.array(unnormalized_probs)\n",
    "    if len(unnormalized_probs) > 0:\n",
    "        normalized_probs = unnormalized_probs / unnormalized_probs.sum()\n",
    "    else:\n",
    "        normalized_probs = unnormalized_probs\n",
    "\n",
    "    return alias_setup(normalized_probs)\n",
    "\n",
    "def preprocess_transition_probs(graph, p=1, q=1):\n",
    "    \"\"\"\n",
    "    preprocess transition probabilities for guiding the random walks.\n",
    "    \"\"\"\n",
    "    alias_nodes = dict()\n",
    "    for node in graph.nodes():\n",
    "        alias_nodes[node] = get_alias_node(graph, node)\n",
    "\n",
    "    alias_edges = dict()\n",
    "    for edge in graph.edges():\n",
    "        alias_edges[edge] = get_alias_edge(graph, edge[0], edge[1], p=p, q=q)\n",
    "\n",
    "    return alias_nodes, alias_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between DeepWalk and node2vec is how to generate random walks. The former only consider the first-order information while the latter also involves the second-order information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_draw(J, q):\n",
    "    \"\"\"\n",
    "    draw sample from a non-uniform discrete distribution using alias sampling.\n",
    "    \"\"\"\n",
    "    K = len(J)\n",
    "\n",
    "    kk = int(np.floor(np.random.rand() * K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]\n",
    "\n",
    "def generate_first_order_random_walk(graph, alias_nodes, walk_length=10, start_node=None):\n",
    "    \"\"\"\n",
    "    simulate a random walk starting from start node and considering the first order information.\n",
    "    \"\"\"\n",
    "    if start_node == None:\n",
    "        start_node = np.random.choice(graph.nodes())\n",
    "    walk = [start_node]\n",
    "    cur = start_node\n",
    "    while len(walk) < walk_length:\n",
    "        cur_nbrs = list(graph.neighbors(cur))\n",
    "        if len(cur_nbrs) > 0:\n",
    "            # sample the next node based on alias_nodes\n",
    "            cur = cur_nbrs[alias_draw(*alias_nodes[cur])]\n",
    "            walk.append(cur)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return walk\n",
    "    \n",
    "def generate_second_order_random_walk(graph, alias_nodes, alias_edges, walk_length=10, start_node=None):\n",
    "    \"\"\"\n",
    "    simulate a random walk starting from start node and considering the second order information.\n",
    "    \"\"\"\n",
    "    if start_node == None:\n",
    "        start_node = np.random.choice(graph.nodes())\n",
    "    walk = [start_node]\n",
    "    \n",
    "    prev = None\n",
    "    cur = start_node\n",
    "    while len(walk) < walk_length:\n",
    "        cur_nbrs = list(graph.neighbors(cur))\n",
    "        if len(cur_nbrs) > 0:\n",
    "            if prev is None:\n",
    "                # sample the next node based on alias_nodes\n",
    "                prev, cur = cur, cur_nbrs[alias_draw(*alias_nodes[cur])]\n",
    "            else:\n",
    "                # sample the next node based on alias_edges\n",
    "                prev, cur = cur, cur_nbrs[alias_draw(*alias_edges[(prev, cur)])]\n",
    "            walk.append(cur)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Embedding Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deepwalk(graph, alias_nodes, node_dim=10, num_walks=10, walk_length=10):\n",
    "    \"\"\"\n",
    "    build a deepwalk model\n",
    "    \"\"\"\n",
    "    print(\"building a DeepWalk model...\", end=\"\\t\")\n",
    "    st = time.time()\n",
    "    np.random.seed(0)\n",
    "    nodes = list(graph.nodes())\n",
    "    walks = list()\n",
    "    # generate random walks\n",
    "    for walk_iter in range(num_walks):\n",
    "        np.random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walks.append(generate_first_order_random_walk(\n",
    "                graph, alias_nodes, walk_length=walk_length, start_node=node))\n",
    "        \n",
    "    walk_lens = [len(w) for w in walks]\n",
    "    if len(walk_lens) > 0:\n",
    "        avg_walk_len = sum(walk_lens) / len(walk_lens)\n",
    "    else:\n",
    "        avg_walk_len = 0.0\n",
    "    print(\"number of walks: %d\\taverage walk length: %.4f\" % (len(walks), avg_walk_len), end=\"\\t\")\n",
    "    \n",
    "    # train a skip-gram model for these walks\n",
    "    model = Word2Vec(walks, size=node_dim, window=3, min_count=0, sg=1, workers=os.cpu_count(), iter=10)\n",
    "    print(\"trainig time: %.4f\" % (time.time()-st))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_node2vec(graph, alias_nodes, alias_edges, node_dim=10, num_walks=10, walk_length=10):\n",
    "    \"\"\"\n",
    "    build a node2vec model\n",
    "    \"\"\"\n",
    "    print(\"building a node2vec model...\", end=\"\\t\")\n",
    "    st = time.time()\n",
    "    np.random.seed(0)\n",
    "    nodes = list(graph.nodes())\n",
    "    walks = list()\n",
    "    # generate random walks\n",
    "    for walk_iter in range(num_walks):\n",
    "        np.random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walks.append(generate_second_order_random_walk(\n",
    "                graph, alias_nodes, alias_edges, walk_length=walk_length, start_node=node))\n",
    "            \n",
    "    walk_lens = [len(w) for w in walks]\n",
    "    if len(walk_lens) > 0:\n",
    "        avg_walk_len = sum(walk_lens) / len(walk_lens)\n",
    "    else:\n",
    "        avg_walk_len = 0.0    \n",
    "    print(\"number of walks: %d\\taverage walk length: %.4f\" % (len(walks), avg_walk_len), end=\"\\t\")\n",
    "    \n",
    "    # train a skip-gram model for these walks\n",
    "    model = Word2Vec(walks, size=node_dim, window=3, min_count=0, sg=1, workers=os.cpu_count(), iter=10)\n",
    "    print(\"trainig time: %.4f\" % (time.time()-st))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sim(model, u, v):\n",
    "    \"\"\"\n",
    "    get the cosine similarity between two nodes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        u = model.wv.vectors[model.wv.index2word.index(u)]\n",
    "        v = model.wv.vectors[model.wv.index2word.index(v)]\n",
    "        return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_auc_score(model, true_edges, false_edges):\n",
    "    \"\"\"\n",
    "    get the auc score\n",
    "    \"\"\"\n",
    "    y_true = [1] * len(true_edges) + [0] * len(false_edges)\n",
    "    \n",
    "    y_score = list()\n",
    "    for e in true_edges:\n",
    "        y_score.append(get_cosine_sim(model, e[0], e[1]))\n",
    "    for e in false_edges:\n",
    "        y_score.append(get_cosine_sim(model, e[0], e[1]))\n",
    "    \n",
    "    return roc_auc_score(y_true, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to load edges into memory and use the networkx.DiGraph structure to store the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_auc_scores = list()\n",
    "# deepwalk_auc_scores.mean()\n",
    "# node2vec_auc_scores.mean()\n",
    "# node_dim\n",
    "# num_walks\n",
    "# walk_length\n",
    "# num_false_edges\n",
    "# p\n",
    "# q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train file:\n",
      "number of nodes: 8347\n",
      "number of edges: 100000\n",
      "In valid file:\n",
      "number of nodes: 5388\n",
      "number of edges: 19268\n"
     ]
    }
   ],
   "source": [
    "user_file = \"data/train.csv\"\n",
    "valid_file = \"data/valid.csv\"\n",
    "print(\"In train file:\")\n",
    "train_edges = load_data(user_file)\n",
    "valid_edges = load_data(valid_file)\n",
    "graph = construct_graph_from_edges(edges)\n",
    "print(\"In valid file:\")\n",
    "valid_graph = construct_graph_from_edges(valid_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link prediction is a task to prediction unseen edges based on graph information. Let's use cross validation to check their performance in the link prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 8347\n",
      "number of edges: 100000\n",
      "building a DeepWalk model...\tnumber of walks: 83470\taverage walk length: 19.2983\ttrainig time: 42.8449\n",
      "building a node2vec model...\tnumber of walks: 83470\taverage walk length: 19.2451\ttrainig time: 42.3727\n",
      "DeepWalk: auc score: 0.9314\n",
      "node2vec: auc score: 0.9310\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# parameters need to be tuned\n",
    "node_dim = 10\n",
    "num_walks = 10\n",
    "walk_length = 22\n",
    "num_false_edges=20732\n",
    "p = 0.5\n",
    "q = 0.5\n",
    "\n",
    "false_edges = generate_false_edges(train_edges+valid_edges,num_false_edges)\n",
    "    \n",
    "# construct the graph and preprocess transition probabilities\n",
    "graph = construct_graph_from_edges(train_edges)\n",
    "alias_nodes, alias_edges = preprocess_transition_probs(graph, p=p, q=q)\n",
    "    \n",
    "\n",
    "# ------------------------------------------ choose one model here--------------------------\n",
    "model1 = build_deepwalk(graph, alias_nodes,\n",
    "                        node_dim=node_dim, num_walks=num_walks, walk_length=walk_length)\n",
    "deepwalk_auc_scores = get_auc_score(model1, valid_edges, false_edges)\n",
    "    \n",
    "model2 = build_node2vec(graph, alias_nodes, alias_edges,\n",
    "                        node_dim=node_dim, num_walks=num_walks, walk_length=walk_length)\n",
    "node2vec_auc_scores = get_auc_score(model2, valid_edges, false_edges)\n",
    "\n",
    "\n",
    "print(\"DeepWalk: auc score: %.4f\" % (deepwalk_auc_scores))\n",
    "print(\"node2vec: auc score: %.4f\" % (node2vec_auc_scores))\n",
    "\n",
    "# store the parameters and scores of this trial\n",
    "vector = (deepwalk_auc_scores,node2vec_auc_scores,node_dim,num_walks,walk_length,num_false_edges,p,q)\n",
    "map_auc_scores.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate valid.csv acccuracy by 'evaluate.py'\n",
    "with open(\"data/valid_ans.csv\",'w',newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"src\",\"dst\",\"score\"])\n",
    "    for edge in valid_edges:\n",
    "        writer.writerow([edge[0],edge[1],1])\n",
    "    for edge in false_edges:\n",
    "        writer.writerow([edge[0],edge[1],0])\n",
    "\n",
    "# generate model prediction\n",
    "with open(\"data/valid_pred.csv\",'w',newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"src\",\"dst\",\"score\"])\n",
    "#------------------------------------------choose model1 or model2 here-----------------------------------------\n",
    "    for edge in valid_edges:\n",
    "        writer.writerow([edge[0],edge[1],get_cosine_sim(model1,edge[0],edge[1])])\n",
    "    for edge in false_edges:\n",
    "        writer.writerow([edge[0],edge[1],get_cosine_sim(model1,edge[0],edge[1])])\n",
    "\n",
    "import os\n",
    "os.system(\"python evaluate.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prediction on test data\n",
    "dataset = pd.read_csv(\"data/test.csv\")\n",
    "src = dataset['src']\n",
    "dst = dataset['dst']\n",
    "\n",
    "with open(\"data/pre.csv\",'w',newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"src\",\"dst\",\"score\"])\n",
    "    for i in range(len(src)):\n",
    "        writer.writerow([src[i],dst[i],get_cosine_sim(model,src[i],dst[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the tune records\n",
    "# remember to change the file name\n",
    "data = {'map_auc_scores':map_auc_scores}\n",
    "f = open('pkl/1.pkl','wb')\n",
    "pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try those functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DBHCFW3mSmmOEpONHVu1rQ',\n",
       " 'gW1l1Ubn4CcKjeJ1fR74aw',\n",
       " 'aomkMAGFrL-gD6cqQjB4bw',\n",
       " 'GqLTKz0YsSkX-DTc9oHrBg',\n",
       " 'tOzTi_wIS2Gajy90pscoMw',\n",
       " 'jbiI4WJrvhE0ia0mEKvz-g',\n",
       " '2xVrxhQJUBmOyG4ML77XKw',\n",
       " '0sOleKBI26BKfpW0SEG6Fw',\n",
       " '0xRAAStEi_dBFLD-1xSMHQ',\n",
       " '3uWCWMWcrn8YSH_qBvlr6w']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alias_nodes, alias_edges = preprocess_transition_probs(graph, p=2, q=2)\n",
    "generate_first_order_random_walk(graph, alias_nodes=alias_nodes,\n",
    "                                 start_node=\"DBHCFW3mSmmOEpONHVu1rQ\", walk_length=10)\n",
    "generate_second_order_random_walk(graph, alias_nodes=alias_nodes, alias_edges=alias_edges,\n",
    "                                  start_node=\"DBHCFW3mSmmOEpONHVu1rQ\", walk_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node embedding (\"DBHCFW3mSmmOEpONHVu1rQ\"): [-0.06658216 -1.7147892   0.11804032 -0.49559236  0.5750869   0.49371615\n",
      "  0.4574471  -0.84304327 -0.8589881  -0.6348172 ]\n",
      "node embedding (\"Fv0e9RIV9jw5TX3ctA1WbA\"): [ 0.12236355 -1.8440742  -0.65914124 -1.2017429  -0.13963051  0.04438686\n",
      " -0.63091016 -0.16266017 -0.24979222 -0.17289004]\n",
      "node embedding (\"VWJ8PSz6Sg5_AlBvQyGvpw\"): [ 0.0982786  -1.8189262  -0.17022008 -0.39542294  0.18176515  0.38275\n",
      " -0.07040039 -1.8763589  -0.67781687 -0.5697088 ]\n",
      "true edge (\"DBHCFW3mSmmOEpONHVu1rQ\", \"Fv0e9RIV9jw5TX3ctA1WbA\"): 0.7338195\n"
     ]
    }
   ],
   "source": [
    "model = build_deepwalk(graph, alias_nodes, node_dim=10, num_walks=5, walk_length=3)\n",
    "model = build_node2vec(graph, alias_nodes, alias_edges, node_dim=10, num_walks=5, walk_length=3)\n",
    "print(\"node embedding (\\\"DBHCFW3mSmmOEpONHVu1rQ\\\"):\",\n",
    "      model.wv.vectors[model.wv.index2word.index(\"DBHCFW3mSmmOEpONHVu1rQ\")])\n",
    "print(\"node embedding (\\\"Fv0e9RIV9jw5TX3ctA1WbA\\\"):\",\n",
    "      model.wv.vectors[model.wv.index2word.index(\"Fv0e9RIV9jw5TX3ctA1WbA\")])\n",
    "print(\"node embedding (\\\"VWJ8PSz6Sg5_AlBvQyGvpw\\\"):\",\n",
    "      model.wv.vectors[model.wv.index2word.index(\"VWJ8PSz6Sg5_AlBvQyGvpw\")])\n",
    "print(\"true edge (\\\"DBHCFW3mSmmOEpONHVu1rQ\\\", \\\"Fv0e9RIV9jw5TX3ctA1WbA\\\"):\",\n",
    "      get_cosine_sim(model, \"N6ZTMIue-2b30CJv2tyPGg\", \"N7E-CfqdME28dakWdEKNvw\"))\n",
    "#print(\"false edge (\\\"N6ZTMIue-2b30CJv2tyPGg\\\", \\\"YwaKGmRNnSa3R3N4Hf9jLw\\\"):\",\n",
    "#      get_cosine_sim(model, \"N6ZTMIue-2b30CJv2tyPGg\", \"YwaKGmRNnSa3R3N4Hf9jLw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interation: 0\n",
      "number of nodes: 8053\n",
      "number of edges: 80000\n",
      "building a DeepWalk model...\tnumber of walks: 80530\taverage walk length: 16.3349\ttrainig time: 41.1549\n",
      "building a node2vec model...\tnumber of walks: 80530\taverage walk length: 16.2764\ttrainig time: 41.0169\n",
      "interation: 1\n",
      "number of nodes: 8080\n",
      "number of edges: 80000\n",
      "building a DeepWalk model...\tnumber of walks: 80800\taverage walk length: 16.0907\ttrainig time: 38.3973\n",
      "building a node2vec model...\tnumber of walks: 80800\taverage walk length: 16.0038\ttrainig time: 38.9277\n",
      "interation: 2\n",
      "number of nodes: 8040\n",
      "number of edges: 80000\n",
      "building a DeepWalk model...\tnumber of walks: 80400\taverage walk length: 16.1958\ttrainig time: 33.1583\n",
      "building a node2vec model...\tnumber of walks: 80400\taverage walk length: 16.1617\ttrainig time: 35.2580\n",
      "interation: 3\n",
      "number of nodes: 8077\n",
      "number of edges: 80000\n",
      "building a DeepWalk model...\tnumber of walks: 80770\taverage walk length: 16.1853\ttrainig time: 34.2953\n",
      "building a node2vec model...\tnumber of walks: 80770\taverage walk length: 16.1200\ttrainig time: 34.4988\n",
      "interation: 4\n",
      "number of nodes: 8076\n",
      "number of edges: 80000\n",
      "building a DeepWalk model...\tnumber of walks: 80760\taverage walk length: 16.2927\ttrainig time: 33.8794\n",
      "building a node2vec model...\tnumber of walks: 80760\taverage walk length: 16.2032\ttrainig time: 34.7192\n",
      "DeepWalk: avg auc score: 0.9206\tstd: 0.0009\n",
      "node2vec: avg auc score: 0.9208\tstd: 0.0008\n",
      "target avg auc score: 0.933%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# parameters need to be tuned\n",
    "kfold = 5\n",
    "node_dim = 10\n",
    "num_walks = 10\n",
    "walk_length = 20\n",
    "num_false_edges=20732\n",
    "p = 0.5\n",
    "q = 0.5\n",
    "\n",
    "# average score from kfold\n",
    "deepwalk_auc_scores = list()\n",
    "node2vec_auc_scores = list()\n",
    "\n",
    "kf = KFold(n_splits=kfold, shuffle=True)\n",
    "for k, (train_idx, valid_idx) in enumerate(kf.split(edges)):\n",
    "    print(\"interation: %d\" %(k))\n",
    "    # generate false_edge\n",
    "    train_edges = [edges[idx] for idx in train_idx]\n",
    "    false_edges = generate_false_edges(train_edges+valid_edges,num_false_edges)\n",
    "    \n",
    "    # construct the graph and preprocess transition probabilities\n",
    "    graph = construct_graph_from_edges(train_edges)\n",
    "    alias_nodes, alias_edges = preprocess_transition_probs(graph, p=p, q=q)\n",
    "    \n",
    "\n",
    "# ------------------------------------------ choose one model here--------------------------\n",
    "    model1 = build_deepwalk(graph, alias_nodes,\n",
    "                           node_dim=node_dim, num_walks=num_walks, walk_length=walk_length)\n",
    "    deepwalk_auc_scores.append(get_auc_score(model1, valid_edges, false_edges))\n",
    "    \n",
    "    model2 = build_node2vec(graph, alias_nodes, alias_edges,\n",
    "                           node_dim=node_dim, num_walks=num_walks, walk_length=walk_length)\n",
    "    node2vec_auc_scores.append(get_auc_score(model2, valid_edges, false_edges))\n",
    "\n",
    "deepwalk_auc_scores = np.array(deepwalk_auc_scores)\n",
    "node2vec_auc_scores = np.array(node2vec_auc_scores)\n",
    "\n",
    "print(\"DeepWalk: avg auc score: %.4f\\tstd: %.4f\" % (deepwalk_auc_scores.mean(), deepwalk_auc_scores.std()))\n",
    "print(\"node2vec: avg auc score: %.4f\\tstd: %.4f\" % (node2vec_auc_scores.mean(), node2vec_auc_scores.std()))\n",
    "print(\"target avg auc score: 0.933%\")\n",
    "\n",
    "# store the parameters and scores of this trial\n",
    "vector = (deepwalk_auc_scores.mean(),node2vec_auc_scores.mean(),node_dim,num_walks,walk_length,num_false_edges,p,q)\n",
    "map_auc_scores.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
